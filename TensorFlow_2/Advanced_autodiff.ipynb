{
 "cells": [
  {
   "source": [
    "# Advanced Automatic Differentiation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)"
   ]
  },
  {
   "source": [
    "## Controlling gradient recording"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(2.0)\n",
    "y = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    x_sq = x * x\n",
    "    with t.stop_recording():\n",
    "        y_sq = y * y\n",
    "    z = x_sq + y_sq\n",
    "\n",
    "grad = t.gradient(z, {'x': x, 'y': y})\n",
    "\n",
    "print('dz/dx:', grad['x'])  # 2*x => 4\n",
    "print('dz/dy:', grad['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(2.0)\n",
    "y = tf.Variable(3.0)\n",
    "reset = True\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    y_sq = y * y\n",
    "    if reset:\n",
    "    # Throw out all the tape recorded so far\n",
    "        t.reset()\n",
    "    z = x * x + y_sq\n",
    "\n",
    "grad = t.gradient(z, {'x': x, 'y': y})\n",
    "\n",
    "print('dz/dx:', grad['x'])  # 2*x => 4\n",
    "print('dz/dy:', grad['y'])"
   ]
  },
  {
   "source": [
    "### Stop gradient"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(2.0)\n",
    "y = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as t:\n",
    "    y_sq = y**2\n",
    "    z = x**2 + tf.stop_gradient(y_sq)\n",
    "\n",
    "grad = t.gradient(z, {'x': x, 'y': y})\n",
    "\n",
    "print('dz/dx:', grad['x'])  # 2*x => 4\n",
    "print('dz/dy:', grad['y'])"
   ]
  },
  {
   "source": [
    "### Custom gradients"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish an identity operation, but clip during the gradient pass\n",
    "@tf.custom_gradient\n",
    "def clip_gradients(y):\n",
    "    def backward(dy):\n",
    "        return tf.clip_by_norm(dy, 0.5)\n",
    "    return y, backward\n",
    "\n",
    "v = tf.Variable(2.0)\n",
    "with tf.GradientTape() as t:\n",
    "    output = clip_gradients(v * v)\n",
    "print(t.gradient(output, v))  # calls \"backward\", which clips 4 to 2"
   ]
  },
  {
   "source": [
    "### Multiple tapes"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = tf.constant(0.0)\n",
    "x1 = tf.constant(0.0)\n",
    "\n",
    "with tf.GradientTape() as tape0, tf.GradientTape() as tape1:\n",
    "    tape0.watch(x0)\n",
    "    tape1.watch(x1)\n",
    "\n",
    "    y0 = tf.math.sin(x0)\n",
    "    y1 = tf.nn.sigmoid(x1)\n",
    "\n",
    "    y = y0 + y1\n",
    "\n",
    "    ys = tf.reduce_sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tape0.gradient(ys, x0).numpy()   # cos(x) => 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tape1.gradient(ys, x1).numpy()   # sigmoid(x1)*(1-sigmoid(x1)) => 0.25"
   ]
  },
  {
   "source": [
    "### Higher-order gradients"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(1.0)  # Create a Tensorflow variable initialized to 1.0\n",
    "\n",
    "with tf.GradientTape() as t2:\n",
    "    with tf.GradientTape() as t1:\n",
    "        y = x * x * x\n",
    "\n",
    "    # Compute the gradient inside the outer `t2` context manager\n",
    "    # which means the gradient computation is differentiable as well.\n",
    "    dy_dx = t1.gradient(y, x)\n",
    "d2y_dx2 = t2.gradient(dy_dx, x)\n",
    "\n",
    "print('dy_dx:', dy_dx.numpy())  # 3 * x**2 => 3.0\n",
    "print('d2y_dx2:', d2y_dx2.numpy())  # 6 * x => 6.0"
   ]
  },
  {
   "source": [
    "#### Example: Input gradient regularization"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([7, 5])\n",
    "\n",
    "layer = tf.keras.layers.Dense(10, activation=tf.nn.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.GradientTape() as t2:\n",
    "    # The inner tape only takes the gradient with respect to the input,\n",
    "    # not the variables.\n",
    "    with tf.GradientTape(watch_accessed_variables=False) as t1:\n",
    "        t1.watch(x)\n",
    "        y = layer(x)\n",
    "        out = tf.reduce_sum(layer(x)**2)\n",
    "    # 1. Calculate the input gradient.\n",
    "    g1 = t1.gradient(out, x)\n",
    "    # 2. Calculate the magnitude of the input gradient.\n",
    "    g1_mag = tf.norm(g1)\n",
    "\n",
    "# 3. Calculate the gradient of the magnitude with respect to the model.\n",
    "dg1_mag = t2.gradient(g1_mag, layer.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[var.shape for var in dg1_mag]"
   ]
  },
  {
   "source": [
    "## Jacobians\n",
    "### Scalar source"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.linspace(-10.0, 10.0, 200+1)\n",
    "delta = tf.Variable(0.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = tf.nn.sigmoid(x+delta)\n",
    "\n",
    "dy_dx = tape.jacobian(y, delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)\n",
    "print(dy_dx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x.numpy(), y, label='y')\n",
    "plt.plot(x.numpy(), dy_dx, label='dy/dx')\n",
    "plt.legend()\n",
    "_ = plt.xlabel('x')"
   ]
  },
  {
   "source": [
    "### Tensor source"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([7, 5])\n",
    "layer = tf.keras.layers.Dense(10, activation=tf.nn.relu)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = layer(x)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = tape.jacobian(y, layer.kernel)\n",
    "j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = tape.gradient(y, layer.kernel)\n",
    "print('g.shape:', g.shape)\n",
    "\n",
    "j_sum = tf.reduce_sum(j, axis=[0, 1])\n",
    "delta = tf.reduce_max(abs(g - j_sum)).numpy()\n",
    "assert delta < 1e-3\n",
    "print('delta:', delta)"
   ]
  },
  {
   "source": [
    "#### Example: Hessian"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([7, 5])\n",
    "layer1 = tf.keras.layers.Dense(8, activation=tf.nn.relu)\n",
    "layer2 = tf.keras.layers.Dense(6, activation=tf.nn.relu)\n",
    "\n",
    "with tf.GradientTape() as t2:\n",
    "    with tf.GradientTape() as t1:\n",
    "        x = layer1(x)\n",
    "        x = layer2(x)\n",
    "        loss = tf.reduce_mean(x**2)\n",
    "\n",
    "    g = t1.gradient(loss, layer1.kernel)\n",
    "\n",
    "h = t2.jacobian(g, layer1.kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'layer.kernel.shape: {layer1.kernel.shape}')\n",
    "print(f'h.shape: {h.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_params = tf.reduce_prod(layer1.kernel.shape)\n",
    "\n",
    "g_vec = tf.reshape(g, [n_params, 1])\n",
    "h_mat = tf.reshape(h, [n_params, n_params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_zero_center(image, **kwargs):\n",
    "    lim = tf.reduce_max(abs(image))\n",
    "    plt.imshow(image, vmin=-lim, vmax=lim, cmap='seismic', **kwargs)\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow_zero_center(h_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-3\n",
    "eye_eps = tf.eye(h_mat.shape[0])*eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X(k+1) = X(k) - (∇²f(X(k)))^-1 @ ∇f(X(k))\n",
    "# h_mat = ∇²f(X(k))\n",
    "# g_vec = ∇f(X(k))\n",
    "update = tf.linalg.solve(h_mat + eye_eps, g_vec)\n",
    "\n",
    "# Reshape the update and apply it to the variable.\n",
    "_ = layer1.kernel.assign_sub(tf.reshape(update, layer1.kernel.shape))"
   ]
  },
  {
   "source": [
    "### Batch Jacobian"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([7, 5])\n",
    "\n",
    "layer1 = tf.keras.layers.Dense(8, activation=tf.nn.elu)\n",
    "layer2 = tf.keras.layers.Dense(6, activation=tf.nn.elu)\n",
    "\n",
    "with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:\n",
    "    tape.watch(x)\n",
    "    y = layer1(x)\n",
    "    y = layer2(y)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = tape.jacobian(y, x)\n",
    "j.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow_zero_center(j[:, 0, :, 0])\n",
    "_ = plt.title('A (batch, batch) slice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_as_patches(j):\n",
    "    # Reorder axes so the diagonals will each form a contiguous patch.\n",
    "    j = tf.transpose(j, [1, 0, 3, 2])\n",
    "    # Pad in between each patch.\n",
    "    lim = tf.reduce_max(abs(j))\n",
    "    j = tf.pad(j, [[0, 0], [1, 1], [0, 0], [1, 1]],\n",
    "                constant_values=-lim)\n",
    "    # Reshape to form a single image.\n",
    "    s = j.shape\n",
    "    j = tf.reshape(j, [s[0]*s[1], s[2]*s[3]])\n",
    "    imshow_zero_center(j, extent=[-0.5, s[2]-0.5, s[0]-0.5, -0.5])\n",
    "\n",
    "plot_as_patches(j)\n",
    "_ = plt.title('All (batch, batch) slices are diagonal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j_sum = tf.reduce_sum(j, axis=2)\n",
    "print(j_sum.shape)\n",
    "j_select = tf.einsum('bxby->bxy', j)\n",
    "print(j_select.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jb = tape.batch_jacobian(y, x)\n",
    "jb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = tf.reduce_max(abs(jb - j_sum))\n",
    "assert error < 1e-3\n",
    "print(error.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([7, 5])\n",
    "\n",
    "layer1 = tf.keras.layers.Dense(8, activation=tf.nn.elu)\n",
    "bn = tf.keras.layers.BatchNormalization()\n",
    "layer2 = tf.keras.layers.Dense(6, activation=tf.nn.elu)\n",
    "\n",
    "with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:\n",
    "    tape.watch(x)\n",
    "    y = layer1(x)\n",
    "    y = bn(y, training=True)\n",
    "    y = layer2(y)\n",
    "\n",
    "j = tape.jacobian(y, x)\n",
    "print(f'j.shape: {j.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_as_patches(j)\n",
    "\n",
    "_ = plt.title('These slices are not diagonal')\n",
    "_ = plt.xlabel(\"Don't use `batch_jacobian`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jb = tape.batch_jacobian(y, x)\n",
    "print(f'jb.shape: {jb.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow (env)",
   "language": "python",
   "name": "tensorflow-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}